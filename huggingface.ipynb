{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python379jvsc74a57bd04ce0e62306dd6a5716965d4519ada776f947e6dfc145b604b11307c10277ef29",
   "display_name": "Python 3.7.9 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9978193640708923}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Allocate a pipeline for sentiment-analysis\n",
    "classifier = pipeline('sentiment-analysis')\n",
    "res = classifier('We are very happy to include pipeline into the transformers repository.')\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForMaskedLM were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['cls.predictions.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[{'sequence': \"[CLS] hello i'm a fashion model. [SEP]\",\n",
       "  'score': 0.10731096565723419,\n",
       "  'token': 4827,\n",
       "  'token_str': 'fashion'},\n",
       " {'sequence': \"[CLS] hello i'm a role model. [SEP]\",\n",
       "  'score': 0.08774472028017044,\n",
       "  'token': 2535,\n",
       "  'token_str': 'role'},\n",
       " {'sequence': \"[CLS] hello i'm a new model. [SEP]\",\n",
       "  'score': 0.05338384583592415,\n",
       "  'token': 2047,\n",
       "  'token_str': 'new'},\n",
       " {'sequence': \"[CLS] hello i'm a super model. [SEP]\",\n",
       "  'score': 0.046672094613313675,\n",
       "  'token': 3565,\n",
       "  'token_str': 'super'},\n",
       " {'sequence': \"[CLS] hello i'm a fine model. [SEP]\",\n",
       "  'score': 0.027095871046185493,\n",
       "  'token': 2986,\n",
       "  'token_str': 'fine'}]"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "unmasker = pipeline('fill-mask', model='bert-base-uncased')\n",
    "unmasker(\"Hello I'm a [MASK] model.\")"
   ]
  },
  {
   "source": [
    "from transformers import BertTokenizer, TFBertModel\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = TFBertModel.from_pretrained(\"bert-base-uncased\")\n",
    "text = \"Replace me by any text you'd like.\"\n",
    "encoded_input = tokenizer(text, return_tensors='tf')\n",
    "output = model(encoded_input)\n",
    "print(output)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 5,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "I0419 15:18:05.192488 19068 filelock.py:274] Lock 1634703104072 acquired on C:\\Users\\15359/.cache\\torch\\transformers\\336363d3718f8cc6432db4a768a053f96a9eae064c8c96aff2bc69fe73929770.4733ec82e81d40e9cf5fd04556267d8958fb150e9339390fc64206b7e5a79c83.h5.lock\n",
      "Downloading: 100%|██████████| 536M/536M [01:27<00:00, 6.12MB/s]\n",
      "I0419 15:19:33.724834 19068 filelock.py:318] Lock 1634703104072 released on C:\\Users\\15359/.cache\\torch\\transformers\\336363d3718f8cc6432db4a768a053f96a9eae064c8c96aff2bc69fe73929770.4733ec82e81d40e9cf5fd04556267d8958fb150e9339390fc64206b7e5a79c83.h5.lock\n"
     ]
    }
   ]
  }
 ]
}