{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python379jvsc74a57bd04ce0e62306dd6a5716965d4519ada776f947e6dfc145b604b11307c10277ef29",
   "display_name": "Python 3.7.9 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([1, 1, 1, 1, 4])\ntensor([[[[1, 2, 3, 4]]]])\n\ntorch.Size([2, 1, 2, 1, 2])\ntorch.Size([2, 2, 2])\ntorch.Size([2, 1, 2, 1, 2])\ntorch.Size([2, 2, 1, 2])\n"
     ]
    }
   ],
   "source": [
    "# torch.Tensor.unsqueeze(dim)\n",
    "# 插入维度1在指定dim上\n",
    "# 同torch.unsqueeze(input, dim) → Tensor\n",
    "import torch\n",
    "\n",
    "x = torch.tensor([[[1, 2, 3, 4]]])\n",
    "print(x.unsqueeze(1).unsqueeze(1).shape)\n",
    "print(x.unsqueeze(0))\n",
    "print()\n",
    "\n",
    "# squeeze 消去dim==1的\n",
    "x = torch.zeros(2, 1, 2, 1, 2)\n",
    "print(x.size())\n",
    "print(torch.squeeze(x).size())\n",
    "print(torch.squeeze(x, 0).size())\n",
    "print(torch.squeeze(x, 1).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "True\nTrue\nFalse\nFalse\nTrue\n"
     ]
    }
   ],
   "source": [
    "# torch.Tensor.contiguous(memory_format=torch.contiguous_format)\n",
    "# Returns a contiguous in memory tensor containing the same data as self tensor. If self tensor is already in the specified memory format, this function returns the self tensor.\n",
    "# view 操作依赖于内存是连续的\n",
    "\n",
    "import torch\n",
    "x = torch.ones(10, 10)\n",
    "print(x.is_contiguous())  # True\n",
    "print(x.view(5, -1).is_contiguous()) # True\n",
    "print(x.view(2, -1).transpose(0, 1).is_contiguous()) # False\n",
    "print(x.transpose(0, 1).is_contiguous())  # False\n",
    "print(x.transpose(0, 1).contiguous().is_contiguous())  # True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.nn.Module.register_buffer\n",
    "# register_buffer(name: str, tensor: Optional[torch.Tensor], persistent: bool = True) → None\n",
    "# Buffers can be accessed as attributes using given names.\n",
    "\n",
    "# torch.nn.Module.register_buffer('running_mean', torch.zeros(num_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'torch.nn.parameter.Parameter'> torch.Size([30, 20])\n<class 'torch.nn.parameter.Parameter'> torch.Size([30])\ntensor([[ 5.6463e-02, -2.0401e-01, -1.5066e-01, -1.5367e-01,  9.4107e-01,\n          7.7045e-02,  6.3558e-01,  4.2444e-01, -2.0648e-01,  1.5113e+00,\n          5.0002e-02, -6.7403e-02,  1.0063e+00, -4.8090e-01,  1.0751e-01,\n          6.6591e-01, -1.1008e-01,  7.8176e-01, -4.5790e-01,  7.9351e-01,\n         -8.5202e-01,  2.3129e-01,  5.6759e-01, -2.3940e-01, -8.3733e-01,\n         -5.4415e-01, -5.2467e-01,  1.4409e-02,  3.3837e-01,  1.4605e-01],\n        [ 4.8078e-02,  1.4130e-03,  9.5987e-01, -3.8910e-01, -8.7307e-02,\n         -7.1647e-01, -1.0594e+00,  3.2038e-01, -8.4332e-01,  3.3733e-01,\n          1.3254e-01, -1.0435e+00, -5.9416e-01, -2.5477e-01,  1.0352e+00,\n          4.7976e-01, -9.0939e-01, -2.8600e-01,  9.9014e-01, -5.1235e-02,\n         -2.1319e+00,  7.8916e-02, -6.5708e-01,  5.7470e-01, -6.3461e-01,\n          4.1828e-02, -1.0421e+00,  1.0265e+00, -4.1915e-01, -1.6266e+00],\n        [-3.3735e-01, -4.2273e-01, -2.1445e-01,  3.6585e-01,  8.2111e-02,\n          5.4934e-01,  6.1411e-01, -8.3055e-01,  2.4493e-01, -2.0685e-01,\n         -1.2097e-01, -5.2952e-01,  7.1854e-02, -1.0505e+00, -9.5171e-02,\n         -1.1195e+00,  8.2418e-01, -3.5440e-01, -7.4206e-01, -7.2443e-01,\n          1.5870e-01,  2.1238e-01,  4.3265e-02,  4.3698e-01,  3.4143e-01,\n          8.0554e-02, -2.1000e-01, -3.9008e-01,  3.4956e-01, -9.4355e-02],\n        [-7.9404e-01, -3.3929e-01,  1.0644e-01, -1.2002e-01, -1.1016e+00,\n         -3.9997e-01, -2.7365e-01,  8.3575e-01, -4.1205e-01,  4.1611e-01,\n          4.8232e-01, -5.9256e-01,  1.0726e-01,  3.3025e-01,  7.9332e-01,\n         -4.1613e-01, -5.1381e-01, -4.2223e-01,  5.8108e-01,  8.2198e-01,\n         -9.4569e-01,  1.0989e+00, -4.9622e-01, -1.4488e-01,  4.0432e-01,\n          6.1091e-01,  6.8925e-01, -2.4139e-01, -1.7232e-01, -2.0221e-01],\n        [ 9.0546e-02,  5.4225e-01, -8.7414e-02,  8.2012e-01,  7.7197e-01,\n         -4.0953e-01,  1.1136e+00, -3.4121e-01,  7.5912e-01, -6.6642e-02,\n          4.2892e-01,  3.1126e-01, -4.8025e-02, -1.3773e+00,  3.4420e-01,\n         -4.0373e-01, -2.1443e-01,  2.2177e-01, -1.1162e+00, -5.9488e-01,\n          5.4244e-01,  7.1219e-01,  2.2333e-01,  5.6155e-01,  5.5783e-01,\n          5.4817e-01, -7.5462e-01,  1.3304e-01, -5.5791e-02, -4.7147e-01],\n        [ 6.6409e-01,  2.2015e-01,  1.1299e+00,  5.3180e-01, -1.1004e+00,\n          1.1853e+00, -4.7595e-01,  9.5765e-01, -5.2036e-01,  1.6647e-01,\n          1.7715e-02,  2.2518e+00,  3.8155e-02,  5.6458e-01, -9.2306e-01,\n          1.3347e-01,  9.7486e-01, -1.0857e-01, -9.2120e-02, -3.2800e-01,\n          1.2240e+00,  3.9138e-01,  1.3542e+00,  4.2571e-01,  9.5031e-02,\n         -9.4263e-01,  1.4020e+00,  9.9601e-01, -2.9039e-01,  9.0159e-01],\n        [-1.5931e-01, -6.6739e-01,  1.2682e+00, -2.1705e-01,  2.3013e-01,\n         -1.3744e-01, -6.9840e-01,  1.6165e+00, -6.1054e-01,  2.2236e-02,\n          9.0875e-01,  7.8152e-01, -2.4649e-01, -3.2529e-01, -4.3948e-01,\n         -3.4143e-01, -8.4844e-01, -1.1158e+00,  5.7128e-01,  7.1671e-01,\n         -7.2349e-01,  1.5687e+00,  5.3597e-01,  5.9587e-01,  2.8424e-01,\n         -3.2012e-01,  7.3973e-01,  2.9262e-01, -9.8873e-01,  4.6140e-01],\n        [ 2.3456e-01, -1.5248e+00,  1.6085e-01, -3.7997e-01, -1.4107e+00,\n         -4.8637e-01, -6.1529e-01, -3.6585e-01, -2.4191e-01,  7.5375e-03,\n          1.4857e-01, -7.6851e-01,  5.6454e-01,  3.5727e-01,  5.2002e-01,\n         -4.6699e-01, -1.7089e-01, -3.5311e-01,  2.9099e-01, -6.1549e-01,\n          2.8613e-02,  1.2785e-01, -5.6323e-01,  5.1795e-03,  1.6894e-01,\n          8.3609e-01,  1.8188e-02,  3.6120e-01,  4.8572e-02, -3.5624e-01],\n        [ 1.8082e-01, -8.0574e-01,  1.6321e+00,  6.2581e-01, -1.3551e+00,\n          1.2213e+00, -2.0228e+00, -7.6546e-01,  2.9691e-01, -7.2770e-01,\n         -7.1916e-01,  7.4711e-01, -1.3001e+00,  8.4718e-01, -3.4790e-01,\n         -1.1375e+00, -1.9140e-01, -6.7117e-01,  7.5342e-01, -5.5144e-01,\n          6.0990e-01,  3.1878e-01, -1.3789e+00,  4.9687e-01, -1.1920e-01,\n          3.1288e-01,  5.4394e-01, -5.3542e-02, -5.2074e-01, -5.6869e-01],\n        [-2.2392e+00,  1.2763e-01, -8.3014e-01,  3.0603e-01, -5.4992e-01,\n          2.4414e-01,  4.6551e-01,  8.4070e-01,  3.9694e-01,  2.3821e-01,\n          7.8547e-01,  3.5714e-01, -3.9938e-02, -4.0017e-01, -2.4369e-01,\n         -4.0586e-01,  2.7680e-01, -6.7526e-01, -1.8007e-02,  9.6628e-01,\n          4.9738e-01,  1.9398e-01,  3.5391e-01, -1.1862e+00,  8.9148e-01,\n         -5.2577e-01,  4.6070e-01, -1.1065e+00,  7.9244e-02,  1.8757e-01]],\n       grad_fn=<AddmmBackward>)\ntorch.Size([10, 30])\ntensor([ 0.0565, -0.2040, -0.1507, -0.1537,  0.9411,  0.0770,  0.6356,  0.4244,\n        -0.2065,  1.5113,  0.0500, -0.0674,  1.0063, -0.4809,  0.1075,  0.6659,\n        -0.1101,  0.7818, -0.4579,  0.7935, -0.8520,  0.2313,  0.5676, -0.2394,\n        -0.8373, -0.5442, -0.5247,  0.0144,  0.3384,  0.1461],\n       grad_fn=<AddBackward0>)\ntorch.Size([30])\n"
     ]
    }
   ],
   "source": [
    "# torch.nn.Module.parameters\n",
    "\n",
    "from torch.nn import Module\n",
    "\n",
    "model = torch.nn.Linear(20, 30)\n",
    "for param in model.parameters():\n",
    "    print(type(param), param.size())\n",
    "\n",
    "vec = torch.randn((10, 20))\n",
    "vec1 = vec[0, :]\n",
    "\n",
    "output = model(vec)\n",
    "print(output)\n",
    "print(output.size())\n",
    "\n",
    "output = model(vec1)\n",
    "print(output)\n",
    "print(output.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[[-2.7287,  0.5007],\n         [ 0.3005,  0.3204],\n         [-0.3785,  0.5644]],\n\n        [[-0.6960,  0.8874],\n         [-0.6072,  1.0363],\n         [-0.1862,  1.3412]]])\ntorch.return_types.max(\nvalues=tensor([[ 0.3005,  0.5644],\n        [-0.1862,  1.3412]]),\nindices=tensor([[1, 2],\n        [2, 2]]))\ntorch.return_types.max(\nvalues=tensor([[-0.6960,  0.8874],\n        [ 0.3005,  1.0363],\n        [-0.1862,  1.3412]]),\nindices=tensor([[1, 1],\n        [0, 1],\n        [1, 1]]))\n"
     ]
    }
   ],
   "source": [
    "# torch.max(input, dim, keepdim)\n",
    "\n",
    "a = torch.randn(2, 3, 2)\n",
    "print(a)\n",
    "print(torch.max(a, 1))\n",
    "print(torch.max(a, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[[ 1.0698, -1.2744],\n         [ 0.1123, -1.2028],\n         [-1.1492, -0.0476]],\n\n        [[ 0.8291,  0.0046],\n         [-0.0686, -1.4995],\n         [-0.5092, -0.6035]]])\ntensor([[[ 1.0698, -1.2744],\n         [ 0.8291,  0.0046]],\n\n        [[ 0.1123, -1.2028],\n         [-0.0686, -1.4995]],\n\n        [[-1.1492, -0.0476],\n         [-0.5092, -0.6035]]])\ntensor(0.1123)\ntensor(0.1123)\ntensor(-0.5092)\ntensor(-0.5092)\n"
     ]
    }
   ],
   "source": [
    "# transpose\n",
    "a = torch.randn(2, 3, 2)\n",
    "print(a)\n",
    "b = a.transpose(0, 1)\n",
    "print(b)\n",
    "\n",
    "print(a[0][1][0])\n",
    "print(b[1][0][0])\n",
    "print(b[2][1][0])\n",
    "print(a[1][2][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[1, 1, 1, 1, 0],\n        [1, 1, 1, 1, 0],\n        [1, 1, 1, 0, 0],\n        [1, 1, 1, 0, 0],\n        [1, 1, 0, 0, 0],\n        [1, 1, 0, 0, 0],\n        [1, 0, 0, 0, 0],\n        [1, 0, 0, 0, 0]], dtype=torch.uint8)\ntorch.Size([8, 5])\ntensor([[[[1, 1, 1, 1, 0]],\n\n         [[1, 1, 1, 1, 0]],\n\n         [[1, 1, 1, 0, 0]],\n\n         [[1, 1, 1, 0, 0]],\n\n         [[1, 1, 0, 0, 0]],\n\n         [[1, 1, 0, 0, 0]],\n\n         [[1, 0, 0, 0, 0]],\n\n         [[1, 0, 0, 0, 0]]]], dtype=torch.uint8)\ntorch.Size([1, 8, 1, 5])\ntensor([[[[False, False, False, False,  True]],\n\n         [[False, False, False, False,  True]],\n\n         [[False, False, False,  True,  True]],\n\n         [[False, False, False,  True,  True]],\n\n         [[False, False,  True,  True,  True]],\n\n         [[False, False,  True,  True,  True]],\n\n         [[False,  True,  True,  True,  True]],\n\n         [[False,  True,  True,  True,  True]]]])\ntorch.Size([1, 8, 1, 5])\n"
     ]
    }
   ],
   "source": [
    "diagonals_list = []\n",
    "d = 2\n",
    "w = 4\n",
    "seq_len = w * d\n",
    "for j in range(-d * w, d, d):\n",
    "    # print(\"j ==\", j)\n",
    "    diagonal_mask = torch.zeros(seq_len, device='cpu', dtype=torch.uint8)\n",
    "    diagonal_mask[:-j] = 1\n",
    "    diagonals_list.append(diagonal_mask)\n",
    "mask = torch.stack(diagonals_list, dim=-1)\n",
    "print(mask)\n",
    "print(mask.shape)\n",
    "mask = mask[None, :, None, :] \n",
    "print(mask)\n",
    "print(mask.shape)\n",
    "mask = mask.flip(dims=(1, 3)).bool()\n",
    "print(mask)\n",
    "print(mask.shape)"
   ]
  }
 ]
}