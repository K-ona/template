{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python379jvsc74a57bd04ce0e62306dd6a5716965d4519ada776f947e6dfc145b604b11307c10277ef29",
   "display_name": "Python 3.7.9 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[9.0919e-39, 8.9082e-39, 9.2755e-39],\n        [8.4490e-39, 1.0194e-38, 9.0919e-39],\n        [8.4490e-39, 1.0745e-38, 1.0102e-38],\n        [9.6429e-39, 8.9082e-39, 9.6429e-39],\n        [1.0102e-38, 9.3673e-39, 9.8265e-39]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.empty(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[0.1610, 0.3348, 0.9130],\n        [0.1299, 0.6322, 0.5556],\n        [0.1420, 0.5830, 0.8455],\n        [0.7507, 0.1554, 0.0799],\n        [0.4318, 0.2021, 0.0160]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "x = torch.zeros(5, 3, dtype = torch.float64)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11], dtype=torch.int32)\ntensor([1, 2])\ntensor(1.)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(np.arange(12))\n",
    "print(x)\n",
    "x = torch.tensor([1, 2])\n",
    "print(x)\n",
    "x = torch.tensor(1.)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[1., 1., 1.],\n        [1., 1., 1.],\n        [1., 1., 1.],\n        [1., 1., 1.],\n        [1., 1., 1.]])\ntensor([[-0.5325,  0.4142,  0.3543],\n        [-1.1940, -2.5324, -2.0227],\n        [-1.2842,  1.6160, -0.5567],\n        [-1.7320, -0.9830, -0.2745],\n        [-0.1649, -0.9282,  1.5012]])\n"
     ]
    }
   ],
   "source": [
    "x = x.new_ones(5, 3)\n",
    "print(x)\n",
    "\n",
    "x = torch.randn_like(x, dtype = torch.float)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([5, 3])\n"
     ]
    }
   ],
   "source": [
    "print(x.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[-0.5325,  0.4142,  0.3543],\n        [-1.1940, -2.5324, -2.0227],\n        [-1.2842,  1.6160, -0.5567],\n        [-1.7320, -0.9830, -0.2745],\n        [-0.1649, -0.9282,  1.5012]])\ntensor([[-0.0188,  0.9581,  0.7263],\n        [-0.4798, -2.1716, -1.2635],\n        [-1.0711,  1.8719,  0.0115],\n        [-1.6489, -0.7230,  0.1634],\n        [ 0.4737, -0.3231,  1.9193]])\ntensor([[-0.5325,  0.4142,  0.3543],\n        [-1.1940, -2.5324, -2.0227],\n        [-1.2842,  1.6160, -0.5567],\n        [-1.7320, -0.9830, -0.2745],\n        [-0.1649, -0.9282,  1.5012]])\n"
     ]
    }
   ],
   "source": [
    "y = torch.rand(5, 3)\n",
    "print(x)\n",
    "print(x.add(y))\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[-0.0188,  0.9581,  0.7263],\n        [-0.4798, -2.1716, -1.2635],\n        [-1.0711,  1.8719,  0.0115],\n        [-1.6489, -0.7230,  0.1634],\n        [ 0.4737, -0.3231,  1.9193]])\n"
     ]
    }
   ],
   "source": [
    "result = torch.empty(5, 3)\n",
    "torch.add(x, y, out = result)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[-1.0128,  0.4708, -1.2474,  0.4702],\n        [ 0.3325, -0.3114,  1.0991,  0.8829],\n        [-1.6745, -2.2039,  0.2103,  0.1087],\n        [ 0.8229,  0.2399, -0.6515,  1.6143]])\ntensor([-1.0128,  0.4708, -1.2474,  0.4702,  0.3325, -0.3114,  1.0991,  0.8829,\n        -1.6745, -2.2039,  0.2103,  0.1087,  0.8229,  0.2399, -0.6515,  1.6143])\ntensor([[-1.0128,  0.4708, -1.2474,  0.4702,  0.3325, -0.3114,  1.0991,  0.8829],\n        [-1.6745, -2.2039,  0.2103,  0.1087,  0.8229,  0.2399, -0.6515,  1.6143]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(4, 4)\n",
    "print(x)\n",
    "y = x.view(16)\n",
    "print(y)\n",
    "z = x.view(2, -1)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[ 0.2844, -0.0453,  0.3944],\n        [ 0.7522,  1.8918,  2.1644]])\n1.891800045967102\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(2, 3)\n",
    "print(x)\n",
    "print(x[1, 1].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[1., 1., 1., 1.],\n        [1., 1., 1., 1.],\n        [1., 1., 1., 1.],\n        [1., 1., 1., 1.]])\n[[1. 1. 1. 1.]\n [1. 1. 1. 1.]\n [1. 1. 1. 1.]\n [1. 1. 1. 1.]]\n<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(4, 4)\n",
    "print(a)\n",
    "b = a.numpy()\n",
    "print(b)\n",
    "print(type(b))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[2., 2., 2., 2.],\n        [2., 2., 2., 2.],\n        [2., 2., 2., 2.],\n        [2., 2., 2., 2.]])\n[[2. 2. 2. 2.]\n [2. 2. 2. 2.]\n [2. 2. 2. 2.]\n [2. 2. 2. 2.]]\n"
     ]
    }
   ],
   "source": [
    "a.add_(1)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[2. 2. 2. 2. 2.]\ntensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.ones(5)\n",
    "b = torch.from_numpy(a)\n",
    "np.add(a, 1, out=a)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[1.2844, 0.9547, 1.3944],\n        [1.7522, 2.8918, 3.1644]], device='cuda:0')\ntensor([[1.2844, 0.9547, 1.3944],\n        [1.7522, 2.8918, 3.1644]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# let us run this cell only if CUDA is available\n",
    "# We will use ``torch.device`` objects to move tensors in and out of GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")          # a CUDA device object\n",
    "    y = torch.ones_like(x, device=device)  # directly create a tensor on GPU\n",
    "    x = x.to(device)                       # or just use strings ``.to(\"cuda\")``\n",
    "    z = x + y\n",
    "    print(z)\n",
    "    print(z.to(\"cpu\", torch.double))       # ``.to`` can also change dtype together!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "False\ntensor([[-6.3716,  6.8664],\n        [11.4320, -1.6055]], requires_grad=True)\nTrue\ntensor(221.0117, grad_fn=<SumBackward0>)\n<SumBackward0 object at 0x000002132B7DB348>\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(2, 2)\n",
    "a = ((a * 3) / (a - 1))\n",
    "print(a.requires_grad)\n",
    "a.requires_grad_(True)\n",
    "print(a)\n",
    "print(a.requires_grad)\n",
    "\n",
    "b = (a * a).sum()\n",
    "print(b)\n",
    "print(b.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[1., 1.],\n        [1., 1.]], requires_grad=True)\nTrue\ntensor([[3., 3.],\n        [3., 3.]], grad_fn=<AddBackward0>)\nTrue\ntensor([[27., 27.],\n        [27., 27.]], grad_fn=<MulBackward0>)\nTrue\ntensor(27., grad_fn=<MeanBackward0>)\nTrue\ntensor([[4.5000, 4.5000],\n        [4.5000, 4.5000]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "print(x)\n",
    "print(x.requires_grad)\n",
    "\n",
    "y = x + 2\n",
    "print(y)\n",
    "print(y.requires_grad)\n",
    "\n",
    "z = y * y * 3\n",
    "print(z)\n",
    "print(z.requires_grad)\n",
    "\n",
    "mean = z.mean()\n",
    "print(mean)\n",
    "print(mean.requires_grad)\n",
    "\n",
    "mean.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([-1.1918,  0.1384, -0.0548], requires_grad=True)\ntensor([-1220.3728,   141.7573,   -56.1410], grad_fn=<MulBackward0>)\ntensor(-378.2522, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, requires_grad=True)\n",
    "print(x)\n",
    "\n",
    "y = x * 2\n",
    "# 默认二范数\n",
    "while y.data.norm() < 1000:\n",
    "    y = y * 2\n",
    "\n",
    "print(y)\n",
    "\n",
    "z = y.mean()\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([-1.1918,  0.1384, -0.0548], requires_grad=True)\ntensor([341.3333, 341.3333, 341.3333])\n"
     ]
    }
   ],
   "source": [
    "print(x)\n",
    "v = torch.tensor([0.1, 1.0, 0.001], dtype = torch.float)\n",
    "# y.backward(v)\n",
    "# print(x.grad)\n",
    "\n",
    "z.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "True\nTrue\nFalse\n"
     ]
    }
   ],
   "source": [
    "print(x.requires_grad)\n",
    "print((x ** 2).requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    print((x ** 2).requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "True\nFalse\ntensor([True, True, True])\ntensor(True)\n"
     ]
    }
   ],
   "source": [
    "print(x.requires_grad)\n",
    "y = x.detach()\n",
    "print(y.requires_grad)\n",
    "print(x == y)\n",
    "print(x.eq(y).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 3)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 3)\n",
    "\n",
    "        self.ff1 = nn.Linear(16 * 6 * 6, 120)\n",
    "        self.ff2 = nn.Linear(120, 84)\n",
    "        self.ff3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "\n",
    "        x = x.view(-1, self.num_flat_featrues(x))\n",
    "        x = F.relu(self.ff1(x))\n",
    "        x = F.relu(self.ff2(x))\n",
    "        x = self.ff3(x)\n",
    "        return x\n",
    "    \n",
    "    def num_flat_featrues(self, x):\n",
    "        size = x.size()[1:]\n",
    "        res = 1\n",
    "        for s in size:\n",
    "            res *= s\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "NeuralNetwork(\n  (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))\n  (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))\n  (ff1): Linear(in_features=576, out_features=120, bias=True)\n  (ff2): Linear(in_features=120, out_features=84, bias=True)\n  (ff3): Linear(in_features=84, out_features=10, bias=True)\n)\n10\ntorch.Size([6, 1, 3, 3])\ntorch.Size([6])\ntorch.Size([16, 6, 3, 3])\ntorch.Size([16])\ntorch.Size([120, 576])\ntorch.Size([120])\ntorch.Size([84, 120])\ntorch.Size([84])\ntorch.Size([10, 84])\ntorch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "net = NeuralNetwork()\n",
    "print(net)\n",
    "\n",
    "params = list(net.parameters())\n",
    "print(params.__len__())\n",
    "for p in params:\n",
    "    print(p.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[ 0.0031, -0.0447,  0.0378,  0.0104,  0.1341,  0.1053, -0.1441,  0.0166,\n         -0.0637, -0.0173]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "input = torch.randn(1, 1, 32, 32)\n",
    "out = net(input)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "net.zero_grad()\n",
    "out.backward(torch.randn(1, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor(1.0274, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "output = net(input)\n",
    "target = torch.randn(10)  # a dummy target, for example\n",
    "target = target.view(1, -1)  # make it the same shape as output\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "loss = criterion(output, target)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<MseLossBackward object at 0x000002132B7D5648>\n<AddmmBackward object at 0x000002132B7D5148>\n<AccumulateGrad object at 0x000002132B7D5648>\n"
     ]
    }
   ],
   "source": [
    "print(loss.grad_fn)  # MSELoss\n",
    "print(loss.grad_fn.next_functions[0][0])  # Linear\n",
    "print(loss.grad_fn.next_functions[0][0].next_functions[0][0])  # ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "conv1.bias.grad before backward\ntensor([0., 0., 0., 0., 0., 0.])\nconv1.bias.grad after backward\ntensor([ 3.4551e-03, -7.1575e-03, -2.2623e-02,  2.0968e-02, -6.1795e-05,\n         4.6398e-03])\n"
     ]
    }
   ],
   "source": [
    "net.zero_grad()     # zeroes the gradient buffers of all parameters\n",
    "\n",
    "print('conv1.bias.grad before backward')\n",
    "print(net.conv1.bias.grad)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print('conv1.bias.grad after backward')\n",
    "print(net.conv1.bias.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[[[-0.2079, -0.1239, -0.2083],\n          [-0.1698, -0.1789, -0.1383],\n          [ 0.2377, -0.2284, -0.2257]]],\n\n\n        [[[-0.0301,  0.2564, -0.1563],\n          [ 0.0481, -0.1358,  0.0810],\n          [ 0.0996,  0.0354,  0.1841]]],\n\n\n        [[[-0.2912,  0.2055, -0.2457],\n          [ 0.0550,  0.1501, -0.1054],\n          [ 0.1912,  0.2574, -0.1901]]],\n\n\n        [[[-0.1409,  0.2050, -0.1754],\n          [ 0.1821,  0.1946,  0.2559],\n          [-0.0241, -0.0960, -0.0494]]],\n\n\n        [[[ 0.3306,  0.3072,  0.2763],\n          [-0.2402, -0.2471,  0.2115],\n          [ 0.1913, -0.2049, -0.1863]]],\n\n\n        [[[ 0.0630,  0.0084, -0.2275],\n          [ 0.2480,  0.1766,  0.2324],\n          [ 0.1345,  0.0245, -0.0695]]]])\ntensor([[[[-0.2095, -0.1137, -0.2211],\n          [-0.1672, -0.1863, -0.1404],\n          [ 0.2364, -0.2223, -0.2086]]],\n\n\n        [[[-0.0174,  0.2728, -0.1637],\n          [ 0.0730, -0.1355,  0.1000],\n          [ 0.1156,  0.0245,  0.1929]]],\n\n\n        [[[-0.3079,  0.2135, -0.2326],\n          [ 0.0545,  0.1613, -0.1358],\n          [ 0.2151,  0.2625, -0.2133]]],\n\n\n        [[[-0.1288,  0.1927, -0.1816],\n          [ 0.1696,  0.1995,  0.2318],\n          [-0.0092, -0.0980, -0.0304]]],\n\n\n        [[[ 0.3522,  0.2975,  0.2728],\n          [-0.2557, -0.2475,  0.2188],\n          [ 0.1879, -0.2011, -0.1803]]],\n\n\n        [[[ 0.0593,  0.0108, -0.2285],\n          [ 0.2304,  0.1816,  0.2399],\n          [ 0.1289,  0.0181, -0.0757]]]])\n"
     ]
    }
   ],
   "source": [
    "lr = 1\n",
    "print(list(net.parameters())[0].data)\n",
    "\n",
    "\n",
    "for f in net.parameters():\n",
    "    f.data.sub_(f.grad.data * lr)\n",
    "\n",
    "print(list(net.parameters())[0].data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "\n",
    "optimizer.zero_grad()\n",
    "output = net(input)\n",
    "loss = criterion(output, target)\n",
    "loss.backward()\n",
    "\n",
    "optimizer.step()"
   ]
  }
 ]
}