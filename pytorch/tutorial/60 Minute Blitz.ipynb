{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit"
  },
  "interpreter": {
   "hash": "4ce0e62306dd6a5716965d4519ada776f947e6dfc145b604b11307c10277ef29"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[1.5072e-09, 7.3428e-43, 1.5072e-09],\n        [7.3428e-43, 1.5073e-09, 7.3428e-43],\n        [1.5073e-09, 7.3428e-43, 1.5072e-09],\n        [7.3428e-43, 1.5072e-09, 7.3428e-43],\n        [1.5072e-09, 7.3428e-43, 1.5072e-09]])\n"
     ]
    }
   ],
   "source": [
    "# 未初始化的tensor\n",
    "x = torch.empty(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[0.0179, 0.5701, 0.4657],\n        [0.0059, 0.5487, 0.1753],\n        [0.7605, 0.7197, 0.1914],\n        [0.9889, 0.4721, 0.3230],\n        [0.5128, 0.3179, 0.0568]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "x = torch.zeros(5, 3, dtype = torch.float64)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11], dtype=torch.int32)\ntensor([1, 2])\ntensor(1.)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(np.arange(12))\n",
    "print(x)\n",
    "x = torch.tensor([1, 2])\n",
    "print(x)\n",
    "x = torch.tensor(1.)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[1., 1., 1.],\n        [1., 1., 1.],\n        [1., 1., 1.],\n        [1., 1., 1.],\n        [1., 1., 1.]])\ntensor([[-0.8730,  0.5166, -0.0898],\n        [-0.7214,  0.8792, -0.2893],\n        [ 1.8547,  2.4534,  1.5781],\n        [-0.4748,  1.1745,  0.1840],\n        [-0.0272, -0.5665,  0.4253]])\n"
     ]
    }
   ],
   "source": [
    "# 基于x返回一个具有相同类型与同样设备的tensor\n",
    "x = x.new_ones(5, 3)\n",
    "print(x)\n",
    "\n",
    "x = torch.randn_like(x, dtype = torch.float)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([5, 3])\n5\n"
     ]
    }
   ],
   "source": [
    "print(x.size())\n",
    "print(x.size(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[-0.8730,  0.5166, -0.0898],\n        [-0.7214,  0.8792, -0.2893],\n        [ 1.8547,  2.4534,  1.5781],\n        [-0.4748,  1.1745,  0.1840],\n        [-0.0272, -0.5665,  0.4253]])\ntensor([[-0.1411,  0.7712,  0.8229],\n        [ 0.1132,  1.0495,  0.2500],\n        [ 1.8597,  3.2619,  1.5954],\n        [ 0.3823,  1.6825,  1.0051],\n        [ 0.8025, -0.4655,  0.8510]])\ntensor([[-0.8730,  0.5166, -0.0898],\n        [-0.7214,  0.8792, -0.2893],\n        [ 1.8547,  2.4534,  1.5781],\n        [-0.4748,  1.1745,  0.1840],\n        [-0.0272, -0.5665,  0.4253]])\n"
     ]
    }
   ],
   "source": [
    "y = torch.rand(5, 3)\n",
    "print(x)\n",
    "print(x.add(y))\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[-0.1411,  0.7712,  0.8229],\n        [ 0.1132,  1.0495,  0.2500],\n        [ 1.8597,  3.2619,  1.5954],\n        [ 0.3823,  1.6825,  1.0051],\n        [ 0.8025, -0.4655,  0.8510]])\n"
     ]
    }
   ],
   "source": [
    "result = torch.empty(5, 3)\n",
    "torch.add(x, y, out = result)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[ 0.4956,  1.2802, -0.8890, -1.4909],\n        [ 0.3764,  1.7616,  0.7685,  1.5485],\n        [-0.9623, -0.0247, -0.5112,  0.1391],\n        [-1.2642,  2.3014, -0.6450,  1.2704]])\ntensor([ 0.4956,  1.2802, -0.8890, -1.4909,  0.3764,  1.7616,  0.7685,  1.5485,\n        -0.9623, -0.0247, -0.5112,  0.1391, -1.2642,  2.3014, -0.6450,  1.2704])\ntensor([[ 0.4956,  1.2802, -0.8890, -1.4909,  0.3764,  1.7616,  0.7685,  1.5485],\n        [-0.9623, -0.0247, -0.5112,  0.1391, -1.2642,  2.3014, -0.6450,  1.2704]])\ntensor([[1., 2.],\n        [3., 4.],\n        [5., 6.]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(4, 4)\n",
    "print(x)\n",
    "y = x.view(16)\n",
    "print(y)\n",
    "z = x.view(2, -1)\n",
    "print(z)\n",
    "\n",
    "a=torch.Tensor([[[1,2,3],[4,5,6]]])\n",
    "print(a.view(3,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[-0.4269,  0.0539,  1.1357],\n        [ 0.5590,  2.1777, -0.2378]])\n2.177673816680908\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(2, 3)\n",
    "print(x)\n",
    "print(x[1, 1].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[1., 1., 1., 1.],\n        [1., 1., 1., 1.],\n        [1., 1., 1., 1.],\n        [1., 1., 1., 1.]])\n[[1. 1. 1. 1.]\n [1. 1. 1. 1.]\n [1. 1. 1. 1.]\n [1. 1. 1. 1.]]\n<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(4, 4)\n",
    "print(a)\n",
    "b = a.numpy()\n",
    "print(b)\n",
    "print(type(b))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[1., 1., 1., 1.],\n        [1., 1., 1., 1.],\n        [1., 1., 1., 1.],\n        [1., 1., 1., 1.]])\n[[1. 1. 1. 1.]\n [1. 1. 1. 1.]\n [1. 1. 1. 1.]\n [1. 1. 1. 1.]]\ntensor([[2., 2., 2., 2.],\n        [2., 2., 2., 2.],\n        [2., 2., 2., 2.],\n        [2., 2., 2., 2.]])\n[[2. 2. 2. 2.]\n [2. 2. 2. 2.]\n [2. 2. 2. 2.]\n [2. 2. 2. 2.]]\n"
     ]
    }
   ],
   "source": [
    "print(a)\n",
    "print(b)\n",
    "a.add_(1)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[2. 2. 2. 2. 2.]\ntensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.ones(5)\n",
    "b = torch.from_numpy(a)\n",
    "np.add(a, 1, out=a)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[0.5731, 1.0539, 2.1357],\n        [1.5590, 3.1777, 0.7622]], device='cuda:0')\ntensor([[0.5731, 1.0539, 2.1357],\n        [1.5590, 3.1777, 0.7622]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# let us run this cell only if CUDA is available\n",
    "# We will use ``torch.device`` objects to move tensors in and out of GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")          # a CUDA device object\n",
    "    y = torch.ones_like(x, device=device)  # directly create a tensor on GPU\n",
    "    x = x.to(device)                       # or just use strings ``.to(\"cuda\")``\n",
    "    z = x + y\n",
    "    print(z)\n",
    "    print(z.to(\"cpu\", torch.double))       # ``.to`` can also change dtype together!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "False\ntensor([[-0.0948,  8.0444],\n        [-0.3139, -2.7368]], requires_grad=True)\nTrue\ntensor([[8.9927e-03, 6.4712e+01],\n        [9.8549e-02, 7.4899e+00]], grad_fn=<MulBackward0>)\ntensor(72.3096, grad_fn=<SumBackward0>)\n<SumBackward0 object at 0x0000020C405BEA48>\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(2, 2)\n",
    "a = ((a * 3) / (a - 1))\n",
    "print(a.requires_grad)\n",
    "a.requires_grad_(True)\n",
    "print(a)\n",
    "print(a.requires_grad)\n",
    "\n",
    "print(a * a)\n",
    "b = (a * a).sum()\n",
    "print(b)\n",
    "print(b.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[1., 1.],\n        [1., 1.]], requires_grad=True)\nTrue\ntensor([[3., 3.],\n        [3., 3.]], grad_fn=<AddBackward0>)\nTrue\ntensor([[27., 27.],\n        [27., 27.]], grad_fn=<MulBackward0>)\nTrue\ntensor(27., grad_fn=<MeanBackward0>)\nTrue\ntensor([[4.5000, 4.5000],\n        [4.5000, 4.5000]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "print(x)\n",
    "print(x.requires_grad)\n",
    "\n",
    "y = x + 2\n",
    "print(y)\n",
    "print(y.requires_grad)\n",
    "\n",
    "z = y * y * 3\n",
    "print(z)\n",
    "print(z.requires_grad)\n",
    "\n",
    "mean = z.mean()\n",
    "print(mean)\n",
    "print(mean.requires_grad)\n",
    "\n",
    "mean.backward()\n",
    "# print(z.grad)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([-0.4424, -0.4969, -0.9364], requires_grad=True)\ntensor([-0.8847, -0.9937, -1.8729], grad_fn=<MulBackward0>)\ntensor([-452.9764, -508.7747, -958.9074], grad_fn=<MulBackward0>)\ntensor(-640.2195, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, requires_grad=True)\n",
    "print(x)\n",
    "\n",
    "y = x * 2\n",
    "print(y)\n",
    "# 默认二范数\n",
    "while y.data.norm() < 1000:\n",
    "    y = 2 * y\n",
    "\n",
    "print(y)\n",
    "\n",
    "z = y.mean()\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([341.3333, 341.3333, 341.3333])\n"
     ]
    }
   ],
   "source": [
    "# v = torch.tensor([0.1, 1.0, 0.001], dtype = torch.float)\n",
    "# y.backward(v, retain_graph=True)\n",
    "# y.backward()\n",
    "z.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "True\nTrue\nFalse\n"
     ]
    }
   ],
   "source": [
    "print(x.requires_grad)\n",
    "print((x ** 2).requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    print((x ** 2).requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([True, True])\ntensor([True, True])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([2., 3.], requires_grad=True)\n",
    "b = torch.tensor([6., 4.], requires_grad=True)\n",
    "\n",
    "Q = 3*a**3 - b**2\n",
    "\n",
    "external_grad = torch.tensor([1., 1.])\n",
    "Q.backward(gradient=external_grad)\n",
    "\n",
    "print(9*a**2 == a.grad)\n",
    "print(-2*b == b.grad)"
   ]
  },
  {
   "source": [
    "<img src='向量雅克比积.png' style=\"zoom: 80%;\">"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Does `a` require gradients? : False\nDoes `b` require gradients?: True\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(5, 5)\n",
    "y = torch.rand(5, 5)\n",
    "z = torch.rand((5, 5), requires_grad=True)\n",
    "\n",
    "a = x + y\n",
    "print(f\"Does `a` require gradients? : {a.requires_grad}\")\n",
    "b = x + z\n",
    "print(f\"Does `b` require gradients?: {b.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ResNet(\n  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu): ReLU(inplace=True)\n  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  (layer1): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer2): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer3): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer4): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n  (fc): Linear(in_features=512, out_features=1000, bias=True)\n)\n"
     ]
    }
   ],
   "source": [
    "from torch import nn, optim\n",
    "import torchvision\n",
    "\n",
    "model = torchvision.models.resnet18(pretrained=True)\n",
    "\n",
    "print(model)\n",
    "# Freeze all the parameters in the network\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# unfrozen by default\n",
    "model.fc = nn.Linear(512, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "False\nFalse\ntensor([[True, True, True, True, True],\n        [True, True, True, True, True],\n        [True, True, True, True, True],\n        [True, True, True, True, True],\n        [True, True, True, True, True]])\n2251642702808\n2250840335688\n2250840297944\ntensor(True)\n"
     ]
    }
   ],
   "source": [
    "print(x.requires_grad)\n",
    "y = x.detach()\n",
    "z = x.data\n",
    "print(y.requires_grad)\n",
    "print(x == y)\n",
    "print(id(x))\n",
    "print(id(y))\n",
    "print(id(z))\n",
    "print(x.eq(y).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 3)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 3)\n",
    "\n",
    "        self.ff1 = nn.Linear(16 * 6 * 6, 120)\n",
    "        self.ff2 = nn.Linear(120, 84)\n",
    "        self.ff3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "\n",
    "        x = x.view(-1, self.num_flat_featrues(x))\n",
    "        x = F.relu(self.ff1(x))\n",
    "        x = F.relu(self.ff2(x))\n",
    "        x = self.ff3(x)\n",
    "        return x\n",
    "    \n",
    "    def num_flat_featrues(self, x):\n",
    "        size = x.size()[1:]\n",
    "        res = 1\n",
    "        for s in size:\n",
    "            res *= s\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "NeuralNetwork(\n  (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))\n  (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))\n  (ff1): Linear(in_features=576, out_features=120, bias=True)\n  (ff2): Linear(in_features=120, out_features=84, bias=True)\n  (ff3): Linear(in_features=84, out_features=10, bias=True)\n)\n10\ntorch.Size([6, 1, 3, 3])\ntorch.Size([6])\ntorch.Size([16, 6, 3, 3])\ntorch.Size([16])\ntorch.Size([120, 576])\ntorch.Size([120])\ntorch.Size([84, 120])\ntorch.Size([84])\ntorch.Size([10, 84])\ntorch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "net = NeuralNetwork()\n",
    "print(net)\n",
    "\n",
    "params = list(net.parameters())\n",
    "print(params.__len__())\n",
    "for p in params:\n",
    "    print(p.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[ 0.0021,  0.0234, -0.1049, -0.1233, -0.1361, -0.0314, -0.0025, -0.1422,\n          0.0288, -0.0501]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "input = torch.randn(1, 1, 32, 32)\n",
    "out = net(input)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "net.zero_grad()\n",
    "out.backward(torch.randn(1, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor(0.4851, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "output = net(input)\n",
    "target = torch.randn(10)  # a dummy target, for example\n",
    "target = target.view(1, -1)  # make it the same shape as output\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "loss = criterion(output, target)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<MseLossBackward object at 0x0000020C405D7648>\n<AddmmBackward object at 0x0000020C405D7548>\n<AccumulateGrad object at 0x0000020C405D7648>\n"
     ]
    }
   ],
   "source": [
    "print(loss.grad_fn)  # MSELoss\n",
    "print(loss.grad_fn.next_functions[0][0])  # Linear\n",
    "print(loss.grad_fn.next_functions[0][0].next_functions[0][0])  # ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "conv1.bias.grad before backward\n",
      "tensor([0., 0., 0., 0., 0., 0.])\n",
      "conv1.bias.grad after backward\n",
      "tensor([ 0.0003, -0.0014, -0.0099,  0.0093, -0.0055,  0.0070])\n"
     ]
    }
   ],
   "source": [
    "net.zero_grad()     # zeroes the gradient buffers of all parameters\n",
    "\n",
    "print('conv1.bias.grad before backward')\n",
    "print(net.conv1.bias.grad)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print('conv1.bias.grad after backward')\n",
    "print(net.conv1.bias.grad)\n",
    "\n",
    "# optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[[[ 8.6246e-02,  1.9947e-02, -3.1505e-02],\n          [-1.4553e-01, -8.7981e-02, -1.0204e-01],\n          [-2.8210e-02,  3.0955e-01, -2.8297e-01]]],\n\n\n        [[[-2.6976e-01,  4.2148e-02,  3.2054e-01],\n          [ 3.7663e-01,  3.1329e-01, -1.8862e-01],\n          [ 1.0169e-01,  8.7619e-02,  2.2490e-01]]],\n\n\n        [[[ 3.3657e-02,  1.7558e-01,  7.8869e-02],\n          [-2.6592e-02,  3.4262e-01,  2.0191e-01],\n          [-2.2809e-02,  9.5029e-02, -3.8280e-01]]],\n\n\n        [[[ 6.2686e-02,  2.2796e-01,  1.6377e-01],\n          [-4.3607e-02, -2.7806e-01,  2.5816e-01],\n          [-3.4707e-01, -1.3352e-01, -4.3442e-02]]],\n\n\n        [[[-2.9319e-01, -4.5864e-02, -3.8315e-02],\n          [-1.5482e-01, -3.1108e-01, -8.4520e-02],\n          [ 2.2495e-01, -6.3066e-02,  1.3796e-02]]],\n\n\n        [[[ 1.4977e-01, -1.2370e-04, -4.8942e-02],\n          [ 3.2611e-01,  1.4503e-03, -2.1540e-01],\n          [-1.6851e-01, -1.7100e-02,  8.8259e-02]]]])\ntensor([[[[ 0.0792,  0.0209, -0.0282],\n          [-0.1467, -0.0878, -0.0979],\n          [-0.0214,  0.3160, -0.2857]]],\n\n\n        [[[-0.2871,  0.0474,  0.3344],\n          [ 0.3945,  0.3271, -0.2038],\n          [ 0.1098,  0.0914,  0.2359]]],\n\n\n        [[[ 0.0344,  0.1818,  0.0821],\n          [-0.0288,  0.3587,  0.2127],\n          [-0.0287,  0.1010, -0.4068]]],\n\n\n        [[[ 0.0689,  0.2390,  0.1806],\n          [-0.0466, -0.2852,  0.2636],\n          [-0.3570, -0.1407, -0.0452]]],\n\n\n        [[[-0.2987, -0.0548, -0.0505],\n          [-0.1526, -0.3173, -0.0907],\n          [ 0.2245, -0.0641,  0.0092]]],\n\n\n        [[[ 0.1515, -0.0037, -0.0462],\n          [ 0.3243,  0.0014, -0.2129],\n          [-0.1751, -0.0247,  0.0878]]]])\n"
     ]
    }
   ],
   "source": [
    "lr = 1\n",
    "print(list(net.parameters())[0].data)\n",
    "# print(net.parameters())\n",
    "\n",
    "for f in net.parameters():\n",
    "    f.data.sub_(f.grad.data * lr)\n",
    "\n",
    "print(list(net.parameters())[0].data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "\n",
    "optimizer.zero_grad()\n",
    "output = net(input)\n",
    "loss = criterion(output, target)\n",
    "loss.backward()\n",
    "\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.ones(5)  # input tensor\n",
    "y = torch.zeros(3)  # expected output\n",
    "w = torch.randn(5, 3, requires_grad=True)\n",
    "b = torch.randn(3, requires_grad=True)\n",
    "z = torch.matmul(x, w)+b\n",
    "loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Gradient function for z = <AddBackward0 object at 0x0000020C034D9908>\n",
      "Gradient function for loss = <BinaryCrossEntropyWithLogitsBackward object at 0x0000020C034D98C8>\n",
      "tensor([[0.1997, 0.0099, 0.3071],\n",
      "        [0.1997, 0.0099, 0.3071],\n",
      "        [0.1997, 0.0099, 0.3071],\n",
      "        [0.1997, 0.0099, 0.3071],\n",
      "        [0.1997, 0.0099, 0.3071]])\n",
      "tensor([0.1997, 0.0099, 0.3071])\n",
      "tensor([[0.3994, 0.0197, 0.6142],\n",
      "        [0.3994, 0.0197, 0.6142],\n",
      "        [0.3994, 0.0197, 0.6142],\n",
      "        [0.3994, 0.0197, 0.6142],\n",
      "        [0.3994, 0.0197, 0.6142]])\n",
      "tensor([0.3994, 0.0197, 0.6142])\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print('Gradient function for z =',z.grad_fn)\n",
    "print('Gradient function for loss =', loss.grad_fn)\n",
    "\n",
    "loss.backward()\n",
    "# We can only perform gradient calculations using backward once on a given graph, for performance reasons. If we need to do several backward calls on the same graph, we need to pass retain_graph=True to the backward call.\n",
    "print(w.grad)\n",
    "print(b.grad)\n",
    "\n",
    "z = torch.matmul(x, w) + b\n",
    "loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)\n",
    "loss.backward()\n",
    "print(w.grad)\n",
    "print(b.grad) \n",
    "# 从结果可以看出 虽然调用了多次backward重新建立了计算图，梯度仍是累加的\n",
    "\n",
    "# print(z.grad) \n",
    "# We can only obtain the grad properties for the leaf nodes of the computational graph, which have requires_grad property set to True. For all other nodes in our graph, gradients will not be available.\n",
    "\n",
    "# By default, all tensors with requires_grad=True are tracking their computational history and support gradient computation. However, there are some cases when we do not need to do that, for example, when we have trained the model and just want to apply it to some input data, i.e. we only want to do forward computations through the network. We can stop tracking computations by surrounding our computation code with torch.no_grad() block:\n",
    "\n",
    "z = torch.matmul(x, w)+b\n",
    "print(z.requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    z = torch.matmul(x, w)+b\n",
    "print(z.requires_grad)\n",
    "\n",
    "\n",
    "\n",
    "# DAGs are dynamic in PyTorch An important thing to note is that the graph is recreated from scratch; after each .backward() call, autograd starts populating a new graph. This is exactly what allows you to use control flow statements in your model; you can change the shape, size and operations at every iteration if needed."
   ]
  }
 ]
}