{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python379jvsc74a57bd04ce0e62306dd6a5716965d4519ada776f947e6dfc145b604b11307c10277ef29",
   "display_name": "Python 3.7.9 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[1.4607e-19, 2.0537e-19, 6.7422e+22],\n        [6.9983e+28, 1.6214e-19, 1.1023e+24],\n        [7.0368e+28, 4.5444e+30, 7.3867e+20],\n        [9.2358e-01, 4.2325e+21, 3.0478e+32],\n        [1.7699e+31, 6.9767e+22, 4.1579e+21]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.empty(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[0.0786, 0.5981, 0.8939],\n        [0.0133, 0.5614, 0.2053],\n        [0.8592, 0.9835, 0.2524],\n        [0.8801, 0.8548, 0.7622],\n        [0.5113, 0.9056, 0.4227]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "x = torch.zeros(5, 3, dtype = torch.float64)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11], dtype=torch.int32)\ntensor([1, 2])\ntensor(1.)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(np.arange(12))\n",
    "print(x)\n",
    "x = torch.tensor([1, 2])\n",
    "print(x)\n",
    "x = torch.tensor(1.)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[1., 1., 1.],\n        [1., 1., 1.],\n        [1., 1., 1.],\n        [1., 1., 1.],\n        [1., 1., 1.]])\ntensor([[ 0.2066, -2.5550,  1.4048],\n        [-1.1621, -0.8741, -1.2657],\n        [-0.2980,  0.1757,  1.2309],\n        [-0.0065,  1.1199, -0.0845],\n        [ 1.7157,  1.7972,  0.5683]])\n"
     ]
    }
   ],
   "source": [
    "x = x.new_ones(5, 3)\n",
    "print(x)\n",
    "\n",
    "x = torch.randn_like(x, dtype = torch.float)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([5, 3])\n"
     ]
    }
   ],
   "source": [
    "print(x.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[ 0.2066, -2.5550,  1.4048],\n        [-1.1621, -0.8741, -1.2657],\n        [-0.2980,  0.1757,  1.2309],\n        [-0.0065,  1.1199, -0.0845],\n        [ 1.7157,  1.7972,  0.5683]])\ntensor([[ 0.9164, -1.7299,  2.3579],\n        [-0.2688, -0.7335, -0.9892],\n        [ 0.0332,  0.6084,  1.4953],\n        [ 0.6096,  1.9239,  0.7592],\n        [ 2.1879,  2.5997,  1.0612]])\ntensor([[ 0.2066, -2.5550,  1.4048],\n        [-1.1621, -0.8741, -1.2657],\n        [-0.2980,  0.1757,  1.2309],\n        [-0.0065,  1.1199, -0.0845],\n        [ 1.7157,  1.7972,  0.5683]])\n"
     ]
    }
   ],
   "source": [
    "y = torch.rand(5, 3)\n",
    "print(x)\n",
    "print(x.add(y))\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[ 0.9164, -1.7299,  2.3579],\n        [-0.2688, -0.7335, -0.9892],\n        [ 0.0332,  0.6084,  1.4953],\n        [ 0.6096,  1.9239,  0.7592],\n        [ 2.1879,  2.5997,  1.0612]])\n"
     ]
    }
   ],
   "source": [
    "result = torch.empty(5, 3)\n",
    "torch.add(x, y, out = result)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[ 0.7895,  1.5117, -0.1419, -0.7127],\n        [ 0.5758,  0.3816, -1.0414,  1.3080],\n        [-0.3776, -0.9746, -0.0952, -0.6751],\n        [-2.3785, -0.0143,  0.2187,  0.7324]])\ntensor([ 0.7895,  1.5117, -0.1419, -0.7127,  0.5758,  0.3816, -1.0414,  1.3080,\n        -0.3776, -0.9746, -0.0952, -0.6751, -2.3785, -0.0143,  0.2187,  0.7324])\ntensor([[ 0.7895,  1.5117, -0.1419, -0.7127,  0.5758,  0.3816, -1.0414,  1.3080],\n        [-0.3776, -0.9746, -0.0952, -0.6751, -2.3785, -0.0143,  0.2187,  0.7324]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(4, 4)\n",
    "print(x)\n",
    "y = x.view(16)\n",
    "print(y)\n",
    "z = x.view(2, -1)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[2.0080, 0.4806, 0.9538],\n        [0.6111, 0.2394, 0.9109]])\n0.23941706120967865\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(2, 3)\n",
    "print(x)\n",
    "print(x[1, 1].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[1., 1., 1., 1.],\n        [1., 1., 1., 1.],\n        [1., 1., 1., 1.],\n        [1., 1., 1., 1.]])\n[[1. 1. 1. 1.]\n [1. 1. 1. 1.]\n [1. 1. 1. 1.]\n [1. 1. 1. 1.]]\n<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(4, 4)\n",
    "print(a)\n",
    "b = a.numpy()\n",
    "print(b)\n",
    "print(type(b))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[2., 2., 2., 2.],\n        [2., 2., 2., 2.],\n        [2., 2., 2., 2.],\n        [2., 2., 2., 2.]])\n[[2. 2. 2. 2.]\n [2. 2. 2. 2.]\n [2. 2. 2. 2.]\n [2. 2. 2. 2.]]\n"
     ]
    }
   ],
   "source": [
    "a.add_(1)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[2. 2. 2. 2. 2.]\ntensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.ones(5)\n",
    "b = torch.from_numpy(a)\n",
    "np.add(a, 1, out=a)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[3.0080, 1.4806, 1.9538],\n        [1.6111, 1.2394, 1.9109]], device='cuda:0')\ntensor([[3.0080, 1.4806, 1.9538],\n        [1.6111, 1.2394, 1.9109]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# let us run this cell only if CUDA is available\n",
    "# We will use ``torch.device`` objects to move tensors in and out of GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")          # a CUDA device object\n",
    "    y = torch.ones_like(x, device=device)  # directly create a tensor on GPU\n",
    "    x = x.to(device)                       # or just use strings ``.to(\"cuda\")``\n",
    "    z = x + y\n",
    "    print(z)\n",
    "    print(z.to(\"cpu\", torch.double))       # ``.to`` can also change dtype together!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "False\ntensor([[ -1.0343, -93.1890],\n        [  1.2444,   0.5944]], requires_grad=True)\nTrue\ntensor(8687.1553, grad_fn=<SumBackward0>)\n<SumBackward0 object at 0x00000245A90EB748>\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(2, 2)\n",
    "a = ((a * 3) / (a - 1))\n",
    "print(a.requires_grad)\n",
    "a.requires_grad_(True)\n",
    "print(a)\n",
    "print(a.requires_grad)\n",
    "\n",
    "b = (a * a).sum()\n",
    "print(b)\n",
    "print(b.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[1., 1.],\n        [1., 1.]], requires_grad=True)\nTrue\ntensor([[3., 3.],\n        [3., 3.]], grad_fn=<AddBackward0>)\nTrue\ntensor([[27., 27.],\n        [27., 27.]], grad_fn=<MulBackward0>)\nTrue\ntensor(27., grad_fn=<MeanBackward0>)\nTrue\ntensor([[4.5000, 4.5000],\n        [4.5000, 4.5000]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "print(x)\n",
    "print(x.requires_grad)\n",
    "\n",
    "y = x + 2\n",
    "print(y)\n",
    "print(y.requires_grad)\n",
    "\n",
    "z = y * y * 3\n",
    "print(z)\n",
    "print(z.requires_grad)\n",
    "\n",
    "mean = z.mean()\n",
    "print(mean)\n",
    "print(mean.requires_grad)\n",
    "\n",
    "mean.backward()\n",
    "# print(z.grad)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([-0.2531,  0.1050, -0.2884], requires_grad=True)\ntensor([-1036.7848,   429.9694, -1181.1854], grad_fn=<MulBackward0>)\ntensor(-596.0003, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, requires_grad=True)\n",
    "print(x)\n",
    "\n",
    "y = x * 2\n",
    "# 默认二范数\n",
    "while y.data.norm() < 1000:\n",
    "    y = y * 2\n",
    "\n",
    "print(y)\n",
    "\n",
    "z = y.mean()\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([-0.2531,  0.1050, -0.2884], requires_grad=True)\ntensor([1365.3334, 1365.3334, 1365.3334])\n"
     ]
    }
   ],
   "source": [
    "print(x)\n",
    "v = torch.tensor([0.1, 1.0, 0.001], dtype = torch.float)\n",
    "# y.backward(v)\n",
    "# print(x.grad)\n",
    "\n",
    "z.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "True\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(x.requires_grad)\n",
    "print((x ** 2).requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    print((x ** 2).requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "True\nFalse\ntensor([True, True, True])\ntensor(True)\n"
     ]
    }
   ],
   "source": [
    "print(x.requires_grad)\n",
    "y = x.detach()\n",
    "print(y.requires_grad)\n",
    "print(x == y)\n",
    "print(x.eq(y).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 3)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 3)\n",
    "\n",
    "        self.ff1 = nn.Linear(16 * 6 * 6, 120)\n",
    "        self.ff2 = nn.Linear(120, 84)\n",
    "        self.ff3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "\n",
    "        x = x.view(-1, self.num_flat_featrues(x))\n",
    "        x = F.relu(self.ff1(x))\n",
    "        x = F.relu(self.ff2(x))\n",
    "        x = self.ff3(x)\n",
    "        return x\n",
    "    \n",
    "    def num_flat_featrues(self, x):\n",
    "        size = x.size()[1:]\n",
    "        res = 1\n",
    "        for s in size:\n",
    "            res *= s\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "NeuralNetwork(\n  (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))\n  (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))\n  (ff1): Linear(in_features=576, out_features=120, bias=True)\n  (ff2): Linear(in_features=120, out_features=84, bias=True)\n  (ff3): Linear(in_features=84, out_features=10, bias=True)\n)\n10\ntorch.Size([6, 1, 3, 3])\ntorch.Size([6])\ntorch.Size([16, 6, 3, 3])\ntorch.Size([16])\ntorch.Size([120, 576])\ntorch.Size([120])\ntorch.Size([84, 120])\ntorch.Size([84])\ntorch.Size([10, 84])\ntorch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "net = NeuralNetwork()\n",
    "print(net)\n",
    "\n",
    "params = list(net.parameters())\n",
    "print(params.__len__())\n",
    "for p in params:\n",
    "    print(p.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[-0.0621, -0.0965, -0.0954,  0.0901,  0.1276,  0.0092, -0.0825,  0.0469,\n         -0.0728,  0.0382]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "input = torch.randn(1, 1, 32, 32)\n",
    "out = net(input)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "net.zero_grad()\n",
    "out.backward(torch.randn(1, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor(1.1337, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "output = net(input)\n",
    "target = torch.randn(10)  # a dummy target, for example\n",
    "target = target.view(1, -1)  # make it the same shape as output\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "loss = criterion(output, target)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<MseLossBackward object at 0x00000245A90FED08>\n<AddmmBackward object at 0x00000245A90FEE88>\n<AccumulateGrad object at 0x00000245A90FED08>\n"
     ]
    }
   ],
   "source": [
    "print(loss.grad_fn)  # MSELoss\n",
    "print(loss.grad_fn.next_functions[0][0])  # Linear\n",
    "print(loss.grad_fn.next_functions[0][0].next_functions[0][0])  # ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "conv1.bias.grad before backward\ntensor([0., 0., 0., 0., 0., 0.])\nconv1.bias.grad after backward\ntensor([ 0.0102, -0.0016, -0.0117, -0.0043,  0.0269, -0.0093])\n"
     ]
    }
   ],
   "source": [
    "net.zero_grad()     # zeroes the gradient buffers of all parameters\n",
    "\n",
    "print('conv1.bias.grad before backward')\n",
    "print(net.conv1.bias.grad)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print('conv1.bias.grad after backward')\n",
    "print(net.conv1.bias.grad)\n",
    "\n",
    "# optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[[[ 0.2530,  0.0687, -0.1647],\n          [-0.1951, -0.1829,  0.1044],\n          [ 0.0726, -0.2835, -0.2274]]],\n\n\n        [[[ 0.2177, -0.0820, -0.0912],\n          [-0.0833, -0.1156, -0.1980],\n          [ 0.0439,  0.1142,  0.2944]]],\n\n\n        [[[ 0.2066, -0.1672, -0.1285],\n          [ 0.0720,  0.3012,  0.1394],\n          [ 0.3020,  0.1312,  0.0606]]],\n\n\n        [[[ 0.1122, -0.3195,  0.1085],\n          [-0.2898,  0.3310, -0.2950],\n          [ 0.1908,  0.0718,  0.0954]]],\n\n\n        [[[-0.0617, -0.1418, -0.0627],\n          [-0.0511, -0.1898,  0.1858],\n          [-0.0183,  0.0347, -0.1260]]],\n\n\n        [[[-0.0945, -0.0590, -0.2707],\n          [ 0.0742,  0.3126, -0.1865],\n          [-0.2247, -0.2625, -0.0322]]]])\ntensor([[[[ 0.2544,  0.0531, -0.1669],\n          [-0.1793, -0.1879,  0.0870],\n          [ 0.0659, -0.2663, -0.2406]]],\n\n\n        [[[ 0.2288, -0.0815, -0.0796],\n          [-0.0813, -0.1085, -0.2074],\n          [ 0.0489,  0.1048,  0.3006]]],\n\n\n        [[[ 0.2196, -0.1772, -0.1347],\n          [ 0.0583,  0.3130,  0.1479],\n          [ 0.3085,  0.1352,  0.0539]]],\n\n\n        [[[ 0.1002, -0.3039,  0.1118],\n          [-0.2904,  0.3302, -0.2907],\n          [ 0.1989,  0.0889,  0.0832]]],\n\n\n        [[[-0.0780, -0.1514, -0.0701],\n          [-0.0419, -0.1757,  0.1591],\n          [-0.0255,  0.0107, -0.1112]]],\n\n\n        [[[-0.0973, -0.0574, -0.2792],\n          [ 0.0789,  0.3151, -0.1714],\n          [-0.2419, -0.2442, -0.0084]]]])\n"
     ]
    }
   ],
   "source": [
    "lr = 1\n",
    "print(list(net.parameters())[0].data)\n",
    "\n",
    "\n",
    "for f in net.parameters():\n",
    "    f.data.sub_(f.grad.data * lr)\n",
    "\n",
    "print(list(net.parameters())[0].data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "\n",
    "optimizer.zero_grad()\n",
    "output = net(input)\n",
    "loss = criterion(output, target)\n",
    "loss.backward()\n",
    "\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.ones(5)  # input tensor\n",
    "y = torch.zeros(3)  # expected output\n",
    "w = torch.randn(5, 3, requires_grad=True)\n",
    "b = torch.randn(3, requires_grad=True)\n",
    "z = torch.matmul(x, w)+b\n",
    "loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Gradient function for z = <AddBackward0 object at 0x00000245DBAFF448>\nGradient function for loss = <BinaryCrossEntropyWithLogitsBackward object at 0x00000245DBAFF108>\ntensor([[0.0137, 0.2763, 0.2907],\n        [0.0137, 0.2763, 0.2907],\n        [0.0137, 0.2763, 0.2907],\n        [0.0137, 0.2763, 0.2907],\n        [0.0137, 0.2763, 0.2907]])\ntensor([0.0137, 0.2763, 0.2907])\ntensor([[0.0273, 0.5525, 0.5815],\n        [0.0273, 0.5525, 0.5815],\n        [0.0273, 0.5525, 0.5815],\n        [0.0273, 0.5525, 0.5815],\n        [0.0273, 0.5525, 0.5815]])\ntensor([0.0273, 0.5525, 0.5815])\ntensor([[0.0410, 0.8288, 0.8722],\n        [0.0410, 0.8288, 0.8722],\n        [0.0410, 0.8288, 0.8722],\n        [0.0410, 0.8288, 0.8722],\n        [0.0410, 0.8288, 0.8722]])\ntensor([0.0410, 0.8288, 0.8722])\nTrue\nFalse\n"
     ]
    }
   ],
   "source": [
    "print('Gradient function for z =',z.grad_fn)\n",
    "print('Gradient function for loss =', loss.grad_fn)\n",
    "\n",
    "loss.backward()\n",
    "# We can only perform gradient calculations using backward once on a given graph, for performance reasons. If we need to do several backward calls on the same graph, we need to pass retain_graph=True to the backward call.\n",
    "print(w.grad)\n",
    "print(b.grad)\n",
    "\n",
    "z = torch.matmul(x, w) + b\n",
    "loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)\n",
    "loss.backward()\n",
    "print(w.grad)\n",
    "print(b.grad) \n",
    "# 从结果可以看出 虽然调用了多次backward重新建立了计算图，梯度仍是累加的\n",
    "\n",
    "# print(z.grad) \n",
    "# We can only obtain the grad properties for the leaf nodes of the computational graph, which have requires_grad property set to True. For all other nodes in our graph, gradients will not be available.\n",
    "\n",
    "# By default, all tensors with requires_grad=True are tracking their computational history and support gradient computation. However, there are some cases when we do not need to do that, for example, when we have trained the model and just want to apply it to some input data, i.e. we only want to do forward computations through the network. We can stop tracking computations by surrounding our computation code with torch.no_grad() block:\n",
    "\n",
    "z = torch.matmul(x, w)+b\n",
    "print(z.requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    z = torch.matmul(x, w)+b\n",
    "print(z.requires_grad)\n",
    "\n",
    "\n",
    "\n",
    "# DAGs are dynamic in PyTorch An important thing to note is that the graph is recreated from scratch; after each .backward() call, autograd starts populating a new graph. This is exactly what allows you to use control flow statements in your model; you can change the shape, size and operations at every iteration if needed."
   ]
  }
 ]
}